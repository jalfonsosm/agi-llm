# Unified LLM Configuration for AGI-Llama
# Copy this file to llm_config.ini and configure for your setup

[common]
# Temperature for extraction (always 0.0 for deterministic results)
temperature_extraction = 0.0

# Base creative temperature for response generation
temperature_creative_base = 0.3

# Random offset for creative temperature variation (0.0-0.3)
temperature_creative_offset = 0.2

# Maximum tokens to generate
max_tokens = 512

# Verbose output (0 or 1)
verbose = 0

personality = Try to keep the message as close as possible to the original.
# personality = Use creativity, humor, sarcasm, and a touch of irreverence.
# personality = Use a dark, somber, and suspenseful tone.
# personality = Try to rhyme the sentences in a poetic tone.
# personality = Use a streetwise, gang-style tone.

[llamacpp]
# Context size
context_size = 4096
batch_size = 1024
u_batch_size = 512
n_threads = 4
top_p = 0.9
top_k = 40
use_gpu = 1
flash_attn = 1
n_seq_max = 8

[bitnet]
# BitNet-specific settings
context_size = 4096
batch_size = 1024
u_batch_size = 512
n_threads = 6
top_p = 0.9
top_k = 40
use_gpu = 0
flash_attn = 0
n_seq_max = 8

[cloud]
# API endpoint (OpenAI-compatible)
api_url = https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct/v1/chat/completions

# API key
api_key = YOUR_API_KEY_HERE

# Model name
model = meta-llama/Llama-3.2-3B-Instruct
