# Cloud LLM Configuration Example
# Copy this to cloud_config.ini and fill in your details

[cloud]
# API endpoint (OpenAI-compatible)
# Examples:
#   OpenAI: https://api.openai.com/v1/chat/completions
#   Azure OpenAI: https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT/chat/completions?api-version=2024-02-15-preview
#   Anthropic (via proxy): https://api.anthropic.com/v1/messages
#   Local (LM Studio): http://localhost:1234/v1/chat/completions
#   Local (Ollama): http://localhost:11434/v1/chat/completions
api_url = https://api.openai.com/v1/chat/completions

# API key
api_key = YOUR_API_KEY_HERE

# Model name
# Examples:
#   OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo
#   Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229
#   Local: depends on loaded model
model = gpt-4

# Temperature (0.0-2.0, higher = more creative)
temperature = 0.7

# Max tokens to generate
max_tokens = 512
