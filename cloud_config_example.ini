# Cloud LLM Configuration

[cloud]
# API endpoint (OpenAI-compatible)
# OpenAI: https://api.openai.com/v1/chat/completions
# Groq: https://api.groq.com/openai/v1/chat/completions
# Cerebras (FREE, fast): https://api.cerebras.ai/v1/chat/completions
# Together AI (FREE trial): https://api.together.xyz/v1/chat/completions
# Hugging Face (FREE): https://api-inference.huggingface.co/models/{model_id}/v1/chat/completions
# Ollama: http://localhost:11434/v1/chat/completions
# ...
api_url = https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct/v1/chat/completions

# API key
api_key = YOUR_API_KEY_HERE

# Model name
# Qwen/Qwen2.5-7B-Instruct, gpt-4, gpt-3.5-turbo, grok-4-latest, etc.
model = meta-llama/Llama-3.2-3B-Instruct

# Temperature (0.0-2.0, higher = more creative)
temperature = 0.7

# Max tokens to generate
max_tokens = 512
