# LLM Configuration - Unified for all backends (llamacpp, bitnet, cloud)

# ============================================================================
# COMMON SETTINGS (used by all backends)
# ============================================================================
[common]
# Extraction temperature (semantic/deterministic) - always 0.0 for accuracy
temperature_extraction = 0.0

# Creative temperature for response generation
temperature_creative_base = 0.3
temperature_creative_offset = 0.2

# Max tokens to generate
max_tokens = 512

# Verbose output (0 = quiet, 1 = verbose)
verbose = 1

personality = Use creativity, humor, sarcasm, and a touch of irreverence.

# ============================================================================
# LLAMACPP BACKEND (local inference with llama.cpp)
# ============================================================================
[llamacpp]
# Context size (tokens)
context_size = 4096

# Batch size
batch_size = 1024

# Ubatch size
u_batch_size = 512

# Number of CPU threads
n_threads = 4

# Top-p sampling
top_p = 0.9

# Top-k sampling
top_k = 40

# Use GPU acceleration (1 = yes, 0 = no)
use_gpu = 1

# Flash attention (1 = yes, 0 = no)
flash_attn = 1

# Max sequences (parallel processing)
n_seq_max = 8

# ============================================================================
# BITNET BACKEND (local inference with BitNet)
# ============================================================================
[bitnet]
# Context size (tokens)
context_size = 2048

# Batch size
batch_size = 512

# Number of CPU threads
n_threads = 4

# Use GPU acceleration (1 = yes, 0 = no)
use_gpu = 1

# ============================================================================
# CLOUD BACKEND (OpenAI-compatible APIs)
# ============================================================================
[cloud]
# API endpoint (OpenAI-compatible)
# OpenAI: https://api.openai.com/v1/chat/completions
# Groq: https://api.groq.com/openai/v1/chat/completions
# Cerebras (FREE): https://api.cerebras.ai/v1/chat/completions
# Together AI: https://api.together.xyz/v1/chat/completions
# HuggingFace: https://api-inference.huggingface.co/models/{model_id}/v1/chat/completions
# Ollama: http://localhost:11434/v1/chat/completions
api_url = https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct/v1/chat/completions

# API key (or use environment variable OPENAI_API_KEY)
api_key = YOUR_API_KEY_HERE

# Model name (gpt-4, gpt-3.5-turbo, meta-llama/Llama-3.2-3B-Instruct, etc.)
model = meta-llama/Llama-3.2-3B-Instruct

# Note: Cloud backend uses temperature_creative_base from [common] section
# Cloud APIs typically use a single temperature value
